---
title: "Exploración de Datos"
author: "joseangeldiazg"
date: "18/12/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introducción.


La práctica final de la asignatura *Introducción a la Ciencia de Datos*, requiere de:

* Aplicación de técnicas de estadística descriptiva y análisis exploratorio de datos sobre los datasets que se estudiarán en los puntos siguientes.
* Estudio de técnicas de clasificación sobre un dataset específico para clasificación. 
* Estudio de técnicas de regresión sobre un dataset específico para regresión. 

En esta primera parte, nos centraremos en el primer punto. Para ello, lo primero que debemos hacer es conocer los datasets, para saber que tipo de datos manejamos y a que contexto o problema real pertenecen. 

### Pima

Los datos de este dataset, corresponden a datos de pacientes proporcionados por el Instituto Nacional de Diabetes, su objetivo, es predecir cuando un paciente tendrá o no diabetes en función de ciertas medidas de diagnóstico. 


### AutoMPG8

Es un dataset usado en problemas regresión. En los que el objetivo está en predecir MPG, *galones por milla*, consumidos por los coches en función de ciertos parámetros. Como su velocidad, peso, aceleración...


## 2. Lectura de los datasets

Los datasets ofrecidos en la web de la asignatura están en formato **Keel**, por lo que antes de proceder a trabajar con ellos deberemos cargarlos en Rstudio. Para ello, procederemos como sigue:


```{r}
pima <- read.csv("../datasets/DatasetsClasificacion/pima/pima.dat", comment.char="@")
autompg8 <- read.csv("../datasets/DatasetsRegresion/autoMPG8/autoMPG8.dat", comment.char="@")
```

Con esto hemos leido los datos, pero tenemos que asignarles nombres a las columnas, ya que por defecto estás no vienen en formato entendible por read.csv, por lo que lo haremos de manera manual. 


```{r}
names(autompg8) <- c("Cylinders", "Displacement", "Horse_power","Weight","Acceleration", "Model_year", "Origin", "Mpg")
names(pima)<-c("Preg", "Plas", "Pres", "Skin", "Insu", "Mass", "Pedi", "Age", "Class")
```

Una vez hecho esto podemos ver el contenido de los datasets, usando la función *head*.

```{r}
head(autompg8)
head(pima)
```

## 3. Exploración del dataset Pima

El primer paso para enfrentarnos a un dataset, será comprender qué es cada una de las variables. En este caso tendremos que usar fuentes de información externas ya que no es fácil de intuir. Tras diversas búsquedas por internet podemos concluir que todo el dataset corresponde a mujeres y las características representan:

**Preg**: Número de embarazos. 
**Plas**: Concentración de plasma de glucosa. 
**Pres**: Presión arterial. 
**Skin**: Espesor de la piel en el triceps en mm. 
**Insu**: Insulina. 
**Mass**: Índice de masa corporal. 
**Pedi**: Función Pedigree de la Diabetes, mide el componente hereditario. 
**Age**:  Edad de la paciente. 
**Class**: Salida, nos dice si tiene o no diabetes.

Una vez conlcuida nuestra investigación podemos centrarnos en los datos "técnicos" del dataset, es decir, frente a que tipos de datos nos encontramos, dimensiones, distribuciones...

### 3.1 Descripción de los datos de entrada del dataset Pima.

Para obtener información sobre el dataset, podemos usar el comando *str* que nos ofrece esta en un formato bastante útil. 

```{r}
str(pima)
```

En base a la salida, podemos comprobar que estamos ante un **dataframe** con 767 observaciones y 9 variables, 8 de ellas son independientes, y una dependiente, Classs, que es un factor con dos niveles. Las variables independientes son de tipo numérico tanto enteros como valores de tipo double.  

Con este simple comando, ya podemos darnos cuenta de un factor importante, y es que tenemos **errores o valores perdidos** en nuestro dataset. Esto es muy claro ya que una persona no puede tener valor 0 en el grosor de la piel, o en insulina. Deberemos estudiar esto con detalle en puntos siguientes, ya que si no lo hacemos y los tomamos como valores a tener en cuenta en el proceso de aprendizaje y harán que nuestro modelo clasifique muy mal ante muestras en test. 

Por otro lado, vamos a comprobar la distribución de la clase, para ello, haremos uso del comando *table*, y de un gráfico de barras para verlo gráficamente. 

```{r}
barplot(table(pima$Class))
```

En base al gráfico podemos concluir que tenemos un problema no balanceado, es decir, con mayor número de representación de una clase que de la otra, por lo que podremos tener en nuestro proceso de clasificación cierto sesgo hacía la clase mayoritaria, en este caso, **tested_negative**.

### 3.2 Calculo de las medidas estadísticas básicas. 

Las medidas estadísticas básicas pueden ser útiles para el proceso de decripción del dataset. Y pueden calcularse fácilmente con funciones como **mean()**, **median()**, **IQR()**... aunque hay paquetes y funciones que nos facilitan esta labor y nos ofrecen más información como **summary()** o **describe()**. 

Usaremos estas, pero antes, pasaremos nuestros valores categorizados como 0 a NA, es decir, valor perdido para que R sepa como tratarlos, debemos tener cuidado en la variable **Preg**, ya que aquí el 0 no es un valor perdido, sino que indica que la paciente nunca ha estado embarazada. 

```{r}
pima[pima==0] <- NA
pima$Preg<-ifelse(is.na(pima$Preg),0,pima$Preg)
summary(pima)
```

Podemos ver como la distribuciones de las medidas de dispersión de las variables exceptuando algunas están en rangos similares,lo que nos facilitará la representación y estudio de las mismas gráficamente, aunque quizá igualmente necesitemos normalizar los datos, ya que hay picos muy altos por arriba en bastantes variables como en **Pedi** o en **Insu**, donde además, tenemos un número tal de valores perdidos que necesitará probablemente ser imputado o predicho con algun método de regresión. 

Por último usaremos la función describe del paquete Hmisc, para ver si podemos obtener más información.

```{r}
library (Hmisc)
describe(pima)
```

### 3.3 Visualización del dataset Pima. 

Una vez llegados a este punto, ya tenemos bastante información sobre los datos con los que estamos trabajando. Pero, intentaremos ir un poco más allá en el análisis del mismo con ciertos gráficos. 

Lo primero será aplicar un **scatter plot** con el que podremos ver si existen correlaciones entre las variables.

```{r}
pairs(~.,data=pima, main="Scatterplot")
```

Parece ser que no hay correlaciones entre las variables, exceptuando **Skin** y **Mass** que parecen estár correlacionadas, algo que es obvio y trivial pues en la mayoría de los casos más masa corporal indicará un mayor grosor de la piel en los puntos de medida. 

Dado que tenemos valores perdidos, usaremos histogramas sobre las variables, además de para hacernos una idea visual de su distribución, para tenerla en cuenta en el punto de imputación de valores perdidos y chequear que esta distribución actual, no se pierde al imputar estos. 

```{r}
par(mfrow = c(2, 2))
hist(pima$Preg)
hist(pima$Plas)
hist(pima$Pres)
hist(pima$Skin)
```



```{r}
par(mfrow = c(2, 2))
hist(pima$Insu)
hist(pima$Mass)
hist(pima$Pedi)
hist(pima$Age)
```

En base a lo anterior, podemos concluir que las variables están distribuidas normalmente en su mayoría, y solo en ciertas variables como **Preg** o **Age**, encontramos distribuciones distintas, algo obvio, al menos en esta variable ya que las edades de las personas en una muestra real no estarán distribuidads normalmente, idem para el número de embarazos. La insulina tampoco está distribuida normalmente y esto si que puede ser un factor a estudiar. 

## 3.4 Preprocesado de datos. 

En este punto trataremos de modificar los datos para obtener mejores resultados o al menos un dataset de mejor calidad para poder pasar a aplicar los algoritmos de clasificación siguientes. 


### Datos unificados

Todos los datos son de tipo entero, por lo que la clase en tipo string puede complicarnos las cosas o dar algun error futuro. Como estámos en un problema binario, lo sustituiremos por 1 o 0. 

```{r}
pima$Class<-ifelse(pima$Class=="tested_positive",1,0)
```


#### Valores perdidos:

El dataset tiene muchos valores perdios. Una posible solución pasaría por eliminar las muestras con estos valores, aunque esto probablemente intensifique en ratio de desbalanceo del problema. 

```{r}
pimacopy<-pima
for (i in 2:6) {
      pimacopy <- pimacopy[-which(is.na(pimacopy[, i])), ]
}
pimacopy
```

Nos hemos quedado con 392 muestras con todos sus datos completos, ahora vamos a ver la distribución de clases. 

```{r}
barplot(table(pimacopy$Class))
```

Parece que el problema de los datos no balanceados no empeora, pero aún así, una mejor solución para el problema de los valores perdidos será la imputación de los mismos mediante un predictor. 


```{r}
library(mice)
mice_mod <- mice(pima[, c("Plas","Pres","Skin","Insu","Mass")], method='rf')
pimacomplete <- complete(mice_mod)
summary(pimacomplete)
```

Podemos ver como ahora no tenemos valores perdidos, pero antes, tal y como dijímos antes, vamos a comprobar que la distribución de variables sigue manteniendose, para ello compararemos los gráficos.

```{r}
par(mfrow=c(2,2))

hist(pima$Plas, freq=F, main='Plas data original',
     col='darkgreen', ylim=c(0,0.04))
hist(pimacomplete$Plas, freq=F, main='Plas datos sin valores perdi',
     col='lightgreen', ylim=c(0,0.04))

hist(pima$Pres, freq=F, main='Pres data original',
     col='darkgreen', ylim=c(0,0.04))
hist(pimacomplete$Pres, freq=F, main='Plas datos sin valores perdidos',
     col='lightgreen', ylim=c(0,0.04))

```


```{r}
par(mfrow=c(2,2))

hist(pima$Skin, freq=F, main='Skin data original',
     col='darkgreen', ylim=c(0,0.04))
hist(pimacomplete$Skin, freq=F, main='Skin datos sin valores perdidos',
     col='lightgreen', ylim=c(0,0.04))

hist(pima$Insu, freq=F, main='Insu data original',
     col='darkgreen', ylim=c(0,0.004))
hist(pimacomplete$Insu, freq=F, main='Insu datos sin valores perdidos',
     col='lightgreen', ylim=c(0,0.004))
```

```{r}
par(mfrow=c(2,2))
hist(pima$Mass, freq=F, main='Mass data original',
     col='darkgreen', ylim=c(0,0.06))
hist(pimacomplete$Mass, freq=F, main='Mass datos sin valores perdidos',
     col='lightgreen', ylim=c(0,0.06))
```

Vemos como las distribuciones se mantienen y por tanto nuestro proceso parece haber funcionado bien. De igual modo, en próximas etapas compararemos el funcionamiento con y sin imputación. Ahora mismo, pasaremos los datos completos al data set con el que estámos trabajando. 

```{r}
pima[,c("Plas","Pres","Skin","Insu","Mass")]<-pimacomplete[,c("Plas","Pres","Skin","Insu","Mass")]
```

Una vez imputados los valores perdidos, podemos volver a comprobar las correlaciones de las variables con el paquete corrplot, a ver si hay cambios o podemos obtener mejor información del dataset que el que obtuvimos en procesos anteriores. 

```{r}
library(corrplot)
M <- cor(pima[c(-9)])
corrplot(M, method = "circle")
```

A parte de la correlación vista anteriormente, entre el **Skin** y **Mass**, tambien podemos ver cierta relación fuerte entre **Insu** y **Plas** con la diferencia de que la distribución de Plas es normal frente a la de Insu, por lo que dejar una en función de otra puede ser apropiado para ciertos algoritmos de clasificación. 

### Normalización

Por último, dado que usaremos algoritmos de clasificación basados en distancias, normalizaremos los datos para evitar problemas en fases posteriores.


```{r}
library(caret)
library(plyr)
normalizados <- preProcess(pima, method = "range")
pima<-predict(normalizados, pima)
head(pima,5)
```


Una vez normalizados, por último usaremos un gráfico de cajas para comprobar si tenemos outliers entre nuestros datos y ver nuevamente las distribuciones de las variables tras la normalización. 


```{r}
boxplot(pima[,c("Preg", "Plas", "Pres","Skin","Insu","Mass","Pedi","Age")])
```

Vemos que tenemos bastantes datos anómalos en nuestro data set y que las distribuciones, salvo en dos o tres casos, son muy desplazadas a la izquierda. 

### 3.4 Resumen y descripción del proceso de Análisis Exploratorio del dataset Pima. 

En este punto, haremos un resumen del estudio de los datos y proceso. En el primer punto hemos estudiado los datos tal y como nos llegan, además de indagar en el significado de cada una de las variables, esto nos ha llevado a encontrar datos anómalos con valor 0, claramente valores perdidos que hemos imputado con una predicción en puntos anteriores, ya que al eliminarlos, el dataset se quedaba muy pobre. Una vez realizada esta imputación, se ha realizado un estudio de las distribuciones, y se ha comparado con las distribuciones de las variables antes de la predición de estos valores pedidos para comprobar que no se han generado valores anómalos.

Hablando de preprocesado, hemos pasado todas la variables a numéricas y normalizado, para favorecer procesos de clasificación basados en distancias y la representacion gráfica de las variables con gráficos de cajas, con el fin de obtener informacion sobre posibles datos anómalos en la muestra. 

En cuanto a la **visualizacion de datos**, aparte de los gráficos anteriormente citados, se han usado: 

- Gráficos de barras para comparar la distribución de clases, con lo que se ha descubierto que estamos ante un problema de datos no balanceados. 
- Histogramas para comparar y ver distribuciones. 
- Scatter Plot y corrplot, para comprobar la correlación de variables. 

## 4. Exploración del dataset autoMPG8.

Al igual que hicimos anteriormente con el dataset **pima**, antes de comenzar a analizar los datos, debemos buscar informacion relativa a estos y su contexto para ubicar el problema en sí. 

**Cylinders**: Es el número de cilindros del coche. 
**Displacement**: Es una medida de longitud, pero no se ha conseguido ubicarla a que corresponde ni en qué unidad está.
**Horse_power**: Son los caballos de potencia del vehículo. 
**Weight**: Es el peso del coche. 
**Acceleration**: Es la aceleración del coche. 
**Model_year**: El año de fabricación del coche. 
**Origin**: Origen del coche, creo que corresponde a regiones.
**Mpg**: Miles Per Gallon, es el consumo del coche, y es la variable dependiente en nuestro problema. La que tendremos que predecir por medio de regresión, debemos tener en cuenta que valores más pequeños de este valor indican más consumos, es decir, con un galón se recorren menos millas. 

Ahora que conocemos algo más del contexto del dataset al que nos enfrentamos, trataremos de focalizarnos en los datos técnicos y del dominio del mismo. 

### 4.1 Descripción de los datos de entrada del dataset autoMPG8.

Como hemos dicho anteriormente, tenemos 8 variables, una dependiente **Mpg** y las demás independientes, para un total de 391 observaciones, es decir, tipos de coche distintos. Los tipos de datos de las variables pueden ser obtenidos con el comando str. 

```{r}
str(autompg8)
```

Todas las variables son númericas y concretamente, **Displacement, Aceleration** y **Mpg**, son de tipo real. Con el comando, describe, podremos ver un poco más en profundidad los datos:


```{r}
describe(autompg8)
```

Vemos que **no tenemos valores perdidos** y que concretamente la variable **Displacement** aunque aparece como tipo num, es un entero, por lo que la cambiaremos, para evitar posibles comportamientos no esperados.

```{r}
autompg8$Displacement<-as.integer(autompg8$Displacement)
```

### 4.2 Cálculo de las medidas estadísticas básicas. 

Vamos a obtener las medidas estadísticas básicas al igual que hicimos con el dataset pima, haciendo uso de funciones que nos ofrecen estas medidas en su conjunto. 

```{r}
summary(autompg8)
```

Vemos que las variables están en rangos distintos por lo que si queremos realizar gráficos conjuntos, probablemente deberemos normalizar las muestras. Hay diferencias muy significativas entre los mínimos y los máximos valores en ciertos casos, lo que puede ser relativo a la presencia de valores anómalos.

### 4.3 Visualización del dataset autoMPG8. 

En un proceso de regresión el primer paso es intentar encontrar gráficamente relaciones entre las variables, para seleccionarlas inicialmente en nuestro proceso. Por ello usaremos scatter plots, dado que en regresión este paso es crucial, usaremos la función ggpairs, en lugar de pairs, ya que nos ofrecerá mucha más información. 

```{r}
library(GGally)
ggpairs(autompg8)
```

Parece ser que tenemos ciertas correlaciones claras en nuestros datos:

- A más caballos de potencia, más peso. 
- A más displacement más peso. 
- A más cilindros más caballos, pesos y displacement...

Y otras correlaciones negativas muy interesantes que pueden ayudarnos a predecir la variable dependiente Mpg. 

- Mayores valores de displacement, horsepower o peso, implican menores valores de **Mpg**, lo que conlleva más consumo, algo bastante obvio pero que sin duda nos ayudará en el proceso de obtención de las variables candidatas para el proceso de regresión. 

Vamos a ver estas correlaciones de una manera un poco más visual:

```{r}
library(corrplot)
M <- cor(autompg8)
corrplot(M, method = "circle")
```

Con colores es mucho más evidente lo que antes se podía ver de manera númerica y sin lugar a dudas tenemos variables muy correlacionadas e interesantes para nuestro proceso de regresión. 

En este punto, que ya tenemos cierta idea de que variables pueden ser interesantes, nos centraremos en realizar algún gráfico más sobre las mismas, para intentar de obtener información relevante para procesos posteriores. Para ello cambiaremos el año, el origen y los cilindros a factor, ya que aunque son variables numéricas nos aportan más potencia a la hora de hacer gráficos de esta manera.

```{r}
autompg8$Origin<-as.factor(autompg8$Origin)
autompg8$Cylinders<-as.factor(autompg8$Cylinders)
autompg8$Model_year<-as.factor(autompg8$Model_year)
```

Primero vamos a intentar ver la distribución del consumo en función del número de cilindros:

```{r}
library(ggplot2)
ggplot(autompg8, aes(Mpg,fill=Cylinders))+geom_histogram(binwidth=1)
```

Vemos que los coches de 4 cilindros además de ser los más comunes, son los que menos consumo ofrecen, por orto lado, los coches de 6 y 8 cilindros consumen bastante. Concretamente, todos los coches cuyo nivel de **Mpg** está por debajo de 16, es decir, consumen mucho, son de 8 cilindros. 

Vamos a ver también, el mismo gráfico en función del año y el origen.  

```{r}
library(ggplot2)
ggplot(autompg8, aes(Mpg,fill=Model_year))+geom_histogram(binwidth=1)
ggplot(autompg8, aes(Mpg,fill=Origin))+geom_histogram(binwidth=1)
```


Podemos ver como los años de los coches se distribuyen en igual medida y como los coches más modernos consumen menos. En cuanto al origin, vemos que el origen mayoritario es de tipo 1 y también son estos coches los que más tendencia tienen a consumos altos frente a los de tipo 2 y 3.

Si superpusieramos ambos gráficos podríamos comprobar como los coches más antiguos se solapan con el origen de tipo 1 y los más nuevos con los origenes 2 y 3. 

Por último vamos a pasar a obtener gráficos de las distribuciones de las variables, ya que es un dato interesante a la hora de hacer predicciones. 


```{r}
par(mfrow = c(2, 2))
hist(as.numeric(autompg8$Cylinders))
hist(autompg8$Displacement)
hist(autompg8$Horse_power)
hist(autompg8$Weight)
```



```{r}
par(mfrow = c(2, 2))
hist(autompg8$Acceleration)
hist(as.numeric(autompg8$Model_year))
hist(as.numeric(autompg8$Origin))
hist(autompg8$Mpg)
```

Vemos como **Acceleration y Mpg**, tienen distribuciones cerca de la normal pero otras como HorsePower o displacement o Weight están desplazadas muy a la izquierda, esto era de esperar ya que habrá pocos coches con mucha potencia o con mucho peso, sino que la mayoría se situará cerca de los valores iniciales. 

Por último realizaremos gráficos de cajas sobre el dataset, para hacernos una última idea de las distribución de sus datos y la presencia o no de posibles outliers. Primero normalizaremos los datos. 

```{r}
normalizados <- preProcess(autompg8, method = "range")
autompg8<-predict(normalizados, autompg8)
boxplot(autompg8[,c( "Displacement", "Horse_power","Weight","Acceleration", "Mpg")])
```

Vemos que están distribuidas bastante homogeneamente y que tenemos tanto outliers por encima como por debajo en las variables de aceleración y caballos de potencia. 

### 4.4 Resumen y descripción del proceso de Análisis Exploratorio del dataset autoMPG8. 


El proceso de análisis exploratorio de datos sobre el dataset **autompg8**, ha sido algo más sencillo que el de pima, debido a que este, no presentaba valores perdidos ni ninguna necesidad especial de pre-procesado de datos aparte de la normalización de los mismos. 

Por medio de gráficos, hemos obtenido información sobre la interación de las variables independientes con la dependiente. Esta información será muy útil de cara al proceso de regresión posterior. 
