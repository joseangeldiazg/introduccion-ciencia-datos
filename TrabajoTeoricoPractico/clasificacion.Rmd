---
title: "Clasificación"
author: "joseangeldiazg"
date: "4/1/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 5 Clasificación 

En este punto, aplicaremos técnicas de clasificación en el dataset pima, que hemos estudiado y procesado en el punto 3, de análisis exploratorio de datos. Nuestro problema, pasará por tanto en predecir, la diabetes sobre nuestro dataset, en función de las variables estudiadas anteriormente y usando las siguientes técnicas:

1. Utilizar el algoritmo k-NN probando con diferentes valores de k. Elegir el que mejor se considere para el problema. 
2. Utilizar el algoritmo LDA para clasificar.
3. Utilizar el algoritmo QDA para clasificar.
4. Comparar los resultados de los tres algoritmos.


El primer paso será por tanto, obtener un conjunto de test y otro de entrenamiento sobre el dataset, aunque en el dataset no hemos apreciado ninguna coherencia en su ordenación, igualmente, aplicaremos sufffle, ya que como estámos ante un problema no balanceado, si no lo hacemos podemos encontrarnos con que sin querer hemos acentuado este problema. 


```{r}
n <- nrow(pima)
shuffled_pima<- pima[sample(n), ]
train_indices <- 1:round(0.8 * n)
train <- shuffled_pima[train_indices, ]
test_indices <- (round(0.8 * n) + 1):n
test <- shuffled_pima[test_indices, ]

str(train)
str(test)
```

En este punto, volveremos a usar un gráfico de barras e histogramas para ver si nuestra division ha sido apropiada. 


```{r}
par(mfrow = c(1, 2))
barplot(table(train$Class), main = "Train Class Distribution")
barplot(table(test$Class),  main = "Test Class Distribution")
```

Vemos como se han dividido bastante bien y no hemos acentuado el problema. Ahora podemos empezar a aplicar técnicas de clasificación, para ello, nos basaremos en el paquete **caret**.


## 5.1 Clasificación con KNN


Al usar este algoritmo uno de los pasos mas complicados es la elección del valor de k apropiado para nuestro modelo. Por suerte, **caret**, incluye la opción del parámetro **tuneGrid** con el que podemos comparara varios valores de k para nuestro modelo y coger automaticamente el mejor. Igualmente, realizaremos un estudio basado en gráficos para ver como se comporta el valor de K, para ello, lo variaremos entre 1 y 20. 
``



```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=10)

x<-train[,-9]
y<-train[,9]
y=as.factor(y)

knnModelk1 <- train(x, y,  method = "knn", trControl=control, tuneGrid = data.frame(.k=1))
knnModelk2 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=2))
knnModelk3 <- train(x, y,  method = "knn", trControl=control, tuneGrid = data.frame(.k=3))
knnModelk4 <- train(x, y,  method = "knn", trControl=control, tuneGrid = data.frame(.k=4))
knnModelk5 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=5))
knnModelk6 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=6))
knnModelk7 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=7))
knnModelk8 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=8))
knnModelk9 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=9))
knnModelk10 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=10))
knnModelk11 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=11))
knnModelk12 <- train(x, y,  method = "knn", trControl=control, tuneGrid = data.frame(.k=12))
knnModelk13 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=13))
knnModelk14 <- train(x, y,  method = "knn", trControl=control, tuneGrid = data.frame(.k=14))
knnModelk15 <- train(x, y,  method = "knn", trControl=control, tuneGrid = data.frame(.k=15))
knnModelk16 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=16))
knnModelk17 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=17))
knnModelk18 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=18))
knnModelk19 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=19))
knnModelk20 <- train(x, y,  method = "knn",  trControl=control, tuneGrid = data.frame(.k=20))
```

Comparamos ahora estadisticamente los resultados para ver cual se ha comportado mejor en las iteraciones del modelo. 

```{r}
results <- resamples(list(K1=knnModelk1, K2=knnModelk2,K3=knnModelk3, K4=knnModelk4, K5=knnModelk5, K6=knnModelk6,K7=knnModelk7, K8=knnModelk8, K9=knnModelk9, K10=knnModelk10, K11=knnModelk11, K12=knnModelk12, K13=knnModelk13, K14=knnModelk14, K15=knnModelk15, K16=knnModelk16, K17=knnModelk17, K18=knnModelk18, K19=knnModelk19, K20=knnModelk20))
summary(results)
```

Al ser tantos modelos entrenados, será mejor usar una representación gráfica para ver el comportamiento de los distintos valores de K. 

```{r}
bwplot(results)
```

Parece ser que K=12 o K=9 tienen buenos resultados, y que los modelos no tienen demasiados outliers ni por encima ni por debabajo, al menos en la evaluación basada en Accuracy, distribuyendose los resultados de una manera bastante homogénea. De igual manera, echa esta comparación entre valores de K, a la hora de entrenar nuestro modelo, dejaremos la seleccion de este valor al parámetro **tuneGrid**.

```{r}
control <- trainControl(method="cv")
knnFinalModel <- train(x, y, method="knn", metric="Accuracy", trControl=control,tuneGrid = data.frame(.k=1:20))
knnFinalModel
```

Vemos como el valor de K escogido por el algorítmo es 11, muy similar a los que habíamos predicho en el proceso de comparación del mejor valor de K. 

Llegados a este punto solo nos quedará usar el conjunto de test para predecir el funcionamiento del modelo creado. 

```{r}
knnPred <- predict(knnFinalModel, newdata = test)
postResample(pred = knnPred, obs = as.factor(test[,9]))
```


Al finalizar nuestro proceso obtenemos un **0.6797386 de accuracy** en test, sin duda podría mejorarse aplicando técnicas de **imbalance learning** o técnicas más robustas de clasificación que el algoritmo KNN, aunque esto lo dejaremos para trabajos futuros. En este punto, retomaremos la partición en la que obviamos todos los **valores perdidos^* para comprobar el el funcionamiento de la predicción de estos.

Vamos a realizar por tanto el proceso de clasificación basado en KNN con el dataset **pimacopy** que tan solo 392 observaciones pero sin valores perdidos y todos los datos son reales, es decir, no hemos realizado predicción de los mismos. 


```{r}
#Dividimos en test y training
n <- nrow(pimacopy)
shuffled_pima<- pimacopy[sample(n), ]
train_indices <- 1:round(0.8 * n)
trainCopy <- shuffled_pima[train_indices, ]
test_indices <- (round(0.8 * n) + 1):n
testCopy <- shuffled_pima[test_indices, ]
str(trainCopy)
str(testCopy)

#Configuramos variables para el modelo

control <- trainControl(method="cv")
x<-trainCopy[,-9]
y<-trainCopy[,9]
y=as.factor(y)

#Entrenamos el modelo

knnFinalModel <- train(x, y, method="knn", metric="Accuracy", trControl=control,tuneGrid = data.frame(.k=1:20))
knnFinalModel
```


```{r}
knnPred <- predict(knnFinalModel, newdata = testCopy)
postResample(pred = knnPred, obs = as.factor(testCopy[,9]))
```

Parece que obtenemos mejores resultados de lo esperado por lo que vamos a indagar más y cambiaremos la evaluación de los dos modelos a evaluación ROC menos sensitiba a los sesgos en problemas no balanceados. Pasaremos a evaluar estos dos modelos basandonos en el área bajo la curva ROC. 


```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=10, classProbs = TRUE, summaryFunction=twoClassSummary)

#Configuramos variables para el modelo

x<-train[,-9]
y<-train[,9]
y=as.factor(y)
levels(y)<-c("X0","X1")

x2<-trainCopy[,-9]
y2<-trainCopy[,9]
y2=as.factor(y2)
levels(y2)<-c("X0","X1")


#Entrenamos los modelos

knnFinalModelPrediccionMV <- train(x, y, method="knn", metric="ROC", trControl=control, tuneGrid = data.frame(.k=1:20))

knnFinalModelEliminacionMV <- train(x2, y2, method="knn", metric="ROC", trControl=control, tuneGrid = data.frame(.k=1:20))

#Los comparamos 

results <- resamples(list(PredicciónValoresPerdidos=knnFinalModelPrediccionMV, EliminaciónValoresPerdidos=knnFinalModelEliminacionMV))
summary(results)
bwplot(results)
```

Vemos como estábamos en lo acertado, y si nos basamos en la evaluación ROC, nuestro proceso de predicción de valores perdidos ha funcionado bastante bien, frente a la eliminación de los mismos. Los buenos resultados obtenidos en por el modelo knn usando solo los 392 muestras sin ningun valor perdido seguramente estén debidos a que la proporción de la clase mayoritaria en test sea alta en relación a la minoritaria y por tanto la evaluación basada en **accuracy** no es la más apropiada. 

## 5.2 LDA 

En este punto vamos a estudiar el método de clasificación LDA. Para ello primero tenemos que comprobar ciertas premisas necesarias para el LDA:

- La muestra es aleatoria: Esto lo damos por hecho. 
- Cada variable predictora está distribuida normalmente.
- Las variables predictoras tienen varianza común. 

Para comprobar si están distribuidas normalmente  usaremos el test de Shapiro-Wilk. Aunque en nuestro proceso de EDA ya tuvimos en cuenta la distribución de las variables, ahora, lo comprobaremos de nuevo. 
```{r}
library(MASS)
library(ISLR)
shapiro.test(pima$Preg)
shapiro.test(pima$Plas)
shapiro.test(pima$Pres)
shapiro.test(pima$Skin)
shapiro.test(pima$Insu)
shapiro.test(pima$Mass)
shapiro.test(pima$Pedi)
```

Los altos valores de W, nos llevan a confirmar la hipótesis de partida del test en la que se considerán los datos distribuidos normalmente. Vamos a confirmarlo usándo los QQ-Plot. 

```{r}
qqnorm(y=pima$Preg)
qqline(y=pima$Preg)

qqnorm(y=pima$Plas)
qqline(y=pima$Plas)

qqnorm(y=pima$Pres)
qqline(y=pima$Pres)

qqnorm(y=pima$Skin)
qqline(y=pima$Skin)

qqnorm(y=pima$Insu)
qqline(y=pima$Insu)

qqnorm(y=pima$Mass)
qqline(y=pima$Mass)
```


Vemos como el único gráfico que se desbia en cierta manera es el de la variable insulina, que fue el que menos valor nos dio en el test de Shapiro-Wilk. Algo que tampoco debería de parecernos fuera de lo normal, ya que tenemos entre manos un problema de predicción de diabetes y la insulina es un factor clave en esta, al menos en su tipo-1 (donde no se produce insulina) y en su tipo-2 (donde se produce insulina pero los organos Diana no son capaces de utilizarla), por lo que puede preveerse y admitirse ciertos valores discordantes en esta variable.  

Por último comparamos que las variables predictoras tienen varianza común. 

```{r}
boxplot(pima[,1:8])
var(pima$Preg)
var(pima$Plas)
var(pima$Pres)
var(pima$Skin)
var(pima$Insu)
var(pima$Mass)
var(pima$Pedi)
var(pima$Age)
```

Aquí podemos tener cierto problema, ya que aunque son varianzas similares, tenemos algunas bastante distintas entre otras. Igualmente, en gran medida pueden confirmarse las premisas de LDA y aplicaremos el método normalmente. 

```{r}
lda.fit <- lda(Class~.,data=train)
lda.fit
plot(lda.fit, type="both")
```

Por último vamos a comprobar como funcionaría en test. 

```{r}
lda.pred <- predict(lda.fit,test)
table(lda.pred$class,test$Class)
mean(lda.pred$class==test$Class)
```

Hemos obtenido un valor aceptable de primeras, pero vamos a probar que pasaría si nos quedaramos solo con algunas variables predictoras, aquellas con varianza más similar. 

```{r}
lda.fit2 <- lda(Class~Pres+Skin+Insu+Mass+Pedi,data=train)
lda.fit2
plot(lda.fit2, type="both")
lda.pred2 <- predict(lda.fit2,test)
table(lda.pred2$class,test$Class)
mean(lda.pred2$class==test$Class)
```

Parece ser que el modelo discrimina peor, constatado queda en los gráficos más similares entre si en este último caso que en el anterior. En todo caso, el modelo no empeora tanto como para haber quitado del mismo variables que parecen ser bastante relevantes, por ello, por medio de QDA vamos a intentar mejorar el resultado ya que este, no toma como premisa que las variables deban tener igual varianza en conjunción, sino que lo hace para una misma clase.  

## 5.3  QDA 

Tal y como hemos dicho anteriormente, antes de aplicar QDA, comprobamos que la varianza entre cada clase sea similar. 

```{r}

print("Varianzas de la clase 1")


var(pima[pima$Class == "1",]$Preg)
var(pima[pima$Class == "1",]$Plas)
var(pima[pima$Class == "1",]$Pres)
var(pima[pima$Class == "1",]$Skin)
var(pima[pima$Class == "1",]$Insu)
var(pima[pima$Class == "1",]$Mass)
var(pima[pima$Class == "1",]$Pedi)
var(pima[pima$Class == "1",]$Age)

print("Varianzas de la clase 0")

var(pima[pima$Class == "0",]$Preg)
var(pima[pima$Class == "0",]$Plas)
var(pima[pima$Class == "0",]$Pres)
var(pima[pima$Class == "0",]$Skin)
var(pima[pima$Class == "0",]$Insu)
var(pima[pima$Class == "0",]$Mass)
var(pima[pima$Class == "0",]$Pedi)
var(pima[pima$Class == "0",]$Age)
```

Podemos comprobar como las variables tienen varianza similares, con alguna discordancia pero en general similares. Por lo que todo apunta a que el método QDA funcionará de manera apropiada, y probablemente mejor que el LDA. 

```{r}
qda.fit <- qda(Class~.,data=train)
qda.fit
qda.pred <- predict(qda.fit,test)
class(qda.pred)
data.frame(qda.pred)
table(qda.pred$class,test$Class)
mean(qda.pred$class==test$Class)
```

Exactamente, tal y como habiamos predicho, el modelo basado en QDA ajusta mejor en nuestro problema que el LDA, ya que las varianzas entre clase, son más similares que en el conjunto del problema. 

## 5.4  Comparación entre los tres métodos

En el último punto vamos a compararar  los tres métodos usados para ver cual de ellos se comporta mejor. Para ello, compararemos los métodos usando los tests de **Wilconxon, Friedman y Holm**. 


Aunque los datos más interesantes son los de test, analizaremos también training para comprobar posibles problemas o fenómenos como sobreajustes. Tomaremos como referencia QDA que parece que nos ha dado buenos resultados. 


```{r}
resultadosTrain <- read.csv("./comparaciones/clasif_train_alumnos.csv")
resultadosTest <- read.csv("./comparaciones/datos.csv")

difsTrain <- (resultadosTrain[,4] - resultadosTrain[,3]) / resultadosTrain[,4]
difsTest <- (resultadosTest[,4] - resultadosTest[,3]) / resultadosTest[,4]

#Datos para train

wilc_knn_ln_train <- cbind(ifelse (difsTrain<0, abs(difsTrain)+0.1, 0+0.1), ifelse (difsTrain>0, abs(difsTrain)+0.1, 0+0.1)) 

colnames(wilc_knn_ln_train) <- c(colnames(resultadosTrain)[4], colnames(resultadosTrain)[3])
head(wilc_knn_ln_train)

#Datos para test

wilc_knn_ln_test <- cbind(ifelse (difsTest<0, abs(difsTest)+0.1, 0+0.1), ifelse (difsTest>0, abs(difsTest)+0.1, 0+0.1)) 

colnames(wilc_knn_ln_test) <- c(colnames(resultadosTest)[4], colnames(resultadosTest)[3])
head(wilc_knn_ln_test)
```


Ahora podemos aplicar el test de Wilconxon. Para ello procedemos de la siguiente manera:


```{r}
QDAvsLDAtra <- wilcox.test(wilc_knn_ln_train[,1], wilc_knn_ln_train[,2], alternative = "two.sided", paired=TRUE) 
QDAvsLDAtst <- wilcox.test(wilc_knn_ln_test[,1], wilc_knn_ln_test[,2], alternative = "two.sided", paired=TRUE) 

RmasTest <- QDAvsLDAtst$statistic
pvalueTest <- QDAvsLDAtst$p.value

RmasTra <- QDAvsLDAtra$statistic
pvalueTra <- QDAvsLDAtra$p.value

QDAvsLDAtra <- wilcox.test(wilc_knn_ln_train[,2], wilc_knn_ln_train[,1], alternative = "two.sided", paired=TRUE) 
QDAvsLDAtst <- wilcox.test(wilc_knn_ln_test[,2], wilc_knn_ln_test[,1], alternative = "two.sided", paired=TRUE) 

RmenosTest <- QDAvsLDAtst$statistic
RmenosTra <-  QDAvsLDAtra$statistic

print("Resultados en Test:")
RmasTest
RmenosTest
pvalueTest

print("Resultados en Training:")
RmasTra
RmenosTra
pvalueTra
```

En test podriamos asegurar que no tenemos diferencias entre los algoritmos, frente a que en training a un 85% de seguridad podriamos concluir que son distintos. 

Vamos a comprar ahora los tres algoritmos usando el test de **Friedman**.

```{r}
test_friedmanTra <- friedman.test(as.matrix(resultadosTrain[,1:3]))
test_friedmanTst <- friedman.test(as.matrix(resultadosTest[,1:3]))
test_friedmanTra
test_friedmanTst
```


Acorde al test de Friedman, y los valores de p-value observados cercanos a 0 tanto en training como en test, podemos concluir que existen diferencias , al menos, entre dos algoritmos de los tres estudiados. En base a esta premisa, usaremos el test de **Holm** para discernir que está ocurriendo con estos algoritmos. 



```{r}
tam <- dim(resultadosTest[,1:3])
groups <- rep(1:tam[2], each=tam[1])
pairwise.wilcox.test(as.matrix(resultadosTest[,2:4]), groups, p.adjust = "holm", paired = TRUE)

tam <- dim(resultadosTrain[,1:3])
groups <- rep(1:tam[2], each=tam[1])
pairwise.wilcox.test(as.matrix(resultadosTrain[,2:4]), groups, p.adjust = "holm", paired = TRUE)
```

Los valores de P-Value tan altos, no aclaran que algoritmo funcionará mejor que otro y aunque se decantan en parte por el QDA (sin ser datos estadisticamente significativos), estos datos varian mucho en training y test, problema que puede ser debido a posibles sobreajustes en el proceso de entrenamiento. 



